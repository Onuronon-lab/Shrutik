"""
Integration tests for export optimization workflow.

This module tests the complete end-to-end export optimization workflow including:
1. Creating chunks with multiple transcriptions
2. Triggering consensus calculation
3. Verifying ready_for_export flag
4. Creating export batches
5. Verifying tar.zst archive contents
6. Testing archive extraction
7. Verifying cleanup operations

NOTE: Some tests require PostgreSQL and are skipped when running with SQLite.
The export batch service uses PostgreSQL's unnest() function to exclude
already-exported chunks, which is not available in SQLite. These tests are
marked with @pytest.mark.skip and include a note about the PostgreSQL requirement.

Tests that work with SQLite:
- Consensus calculation workflow
- Bulk operations performance tests
- Index usage verification

Tests that require PostgreSQL:
- Complete export workflow (with batch creation)
- Storage backend switching tests
"""

import json
import os
import shutil
import tarfile
import tempfile
from datetime import datetime, timezone
from unittest.mock import Mock, patch

import pytest
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from app.core.config import StorageConfig
from app.core.security import get_password_hash
from app.db.database import Base
from app.models.audio_chunk import AudioChunk
from app.models.export_batch import ExportBatchStatus, StorageType
from app.models.language import Language
from app.models.script import DurationCategory, Script
from app.models.transcription import Transcription
from app.models.user import User, UserRole
from app.models.voice_recording import RecordingStatus, VoiceRecording
from app.services.consensus_service import ConsensusService
from app.services.export_batch_service import ExportBatchService

# Create test database
SQLALCHEMY_DATABASE_URL = "sqlite:///./test_export_optimization_integration.db"
engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


@pytest.fixture(scope="function")
def db_session():
    """Create a test database session."""
    Base.metadata.create_all(bind=engine)
    session = TestingSessionLocal()
    try:
        yield session
    finally:
        session.close()
        Base.metadata.drop_all(bind=engine)


@pytest.fixture
def test_user(db_session):
    """Create a test user."""
    user = User(
        name="Test User",
        email="test@example.com",
        password_hash=get_password_hash("testpass123"),
        role=UserRole.CONTRIBUTOR,
    )
    db_session.add(user)
    db_session.commit()
    db_session.refresh(user)
    return user


@pytest.fixture
def test_language(db_session):
    """Create a test language."""
    language = Language(name="Bengali", code="bn")
    db_session.add(language)
    db_session.commit()
    db_session.refresh(language)
    return language


@pytest.fixture
def test_script(db_session, test_language):
    """Create a test script."""
    script = Script(
        text="Test script for export optimization",
        duration_category=DurationCategory.SHORT,
        language_id=test_language.id,
    )
    db_session.add(script)
    db_session.commit()
    db_session.refresh(script)
    return script


@pytest.fixture
def temp_export_dir():
    """Create a temporary directory for export testing."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Cleanup
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)


@pytest.fixture
def temp_audio_dir():
    """Create a temporary directory for audio files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    # Cleanup
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)


class TestEndToEndExportWorkflow:
    """Test complete end-to-end export workflow."""

    def test_consensus_calculation_workflow(
        self, db_session, test_user, test_language, test_script, temp_audio_dir
    ):
        """
        Test the consensus calculation workflow (steps 1-3):
        1. Create chunks with 5+ transcriptions
        2. Trigger consensus calculation
        3. Verify ready_for_export flag set
        """
        print("\n=== Testing Consensus Calculation Workflow ===")

        # Step 1: Create chunks with 5+ transcriptions
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "recording.webm"),
            duration=20.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        chunks = []
        for i in range(3):
            audio_file_path = os.path.join(temp_audio_dir, f"chunk_{i}.webm")
            with open(audio_file_path, "wb") as f:
                f.write(b"WEBM_DUMMY_AUDIO_DATA_" + str(i).encode() * 100)

            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=audio_file_path,
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                sentence_hint=f"Test sentence {i}",
                transcript_count=0,
                ready_for_export=False,
                consensus_quality=0.0,
            )
            db_session.add(chunk)
            db_session.commit()
            db_session.refresh(chunk)
            chunks.append(chunk)

            # Create 5 similar transcriptions for each chunk
            for j in range(5):
                transcription = Transcription(
                    chunk_id=chunk.id,
                    user_id=test_user.id,
                    language_id=test_language.id,
                    text=f"This is test transcription {i}.",
                    quality=0.95,
                    confidence=0.92,
                    is_consensus=False,
                    is_validated=False,
                )
                db_session.add(transcription)

            chunk.transcript_count = 5
            db_session.commit()

        print(f"Created {len(chunks)} chunks with 5 transcriptions each")

        # Step 2: Trigger consensus calculation
        consensus_service = ConsensusService(db_session)
        chunk_ids = [chunk.id for chunk in chunks]
        results = consensus_service.calculate_consensus_for_chunks(chunk_ids)

        print(f"Consensus calculation results: {results}")

        # Step 3: Verify ready_for_export flag set
        for chunk in chunks:
            db_session.refresh(chunk)
            assert (
                chunk.ready_for_export is True
            ), f"Chunk {chunk.id} not marked ready for export"
            assert (
                chunk.consensus_quality >= 0.90
            ), f"Chunk {chunk.id} quality too low: {chunk.consensus_quality}"
            assert (
                chunk.consensus_transcript_id is not None
            ), f"Chunk {chunk.id} has no consensus transcript"
            print(
                f"âœ“ Chunk {chunk.id}: ready={chunk.ready_for_export}, quality={chunk.consensus_quality:.2f}"
            )

        print("Consensus calculation workflow test PASSED")

    @pytest.mark.skip(
        reason="Requires PostgreSQL - SQLite doesn't support unnest() function used in export batch query"
    )
    def test_complete_export_workflow(
        self,
        db_session,
        test_user,
        test_language,
        test_script,
        temp_export_dir,
        temp_audio_dir,
    ):
        """
        Test the complete export workflow:
        1. Create chunks with 5+ transcriptions
        2. Trigger consensus calculation
        3. Verify ready_for_export flag set
        4. Create export batch
        5. Verify tar.zst archive contents
        6. Test archive extraction
        7. Verify cleanup

        NOTE: This test requires PostgreSQL as it uses the unnest() function
        to exclude already-exported chunks. SQLite does not support this function.
        """
        # Step 1: Create chunks with 5+ transcriptions
        print("\n=== Step 1: Creating chunks with transcriptions ===")

        # Create voice recording
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "recording.webm"),
            duration=20.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()
        db_session.refresh(recording)

        # Create audio chunks with actual audio files
        chunks = []
        for i in range(3):
            # Create dummy audio file
            audio_file_path = os.path.join(temp_audio_dir, f"chunk_{i}.webm")
            with open(audio_file_path, "wb") as f:
                f.write(b"WEBM_DUMMY_AUDIO_DATA_" + str(i).encode() * 100)

            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=audio_file_path,
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                sentence_hint=f"Test sentence {i}",
                transcript_count=0,
                ready_for_export=False,
                consensus_quality=0.0,
            )
            db_session.add(chunk)
            db_session.commit()
            db_session.refresh(chunk)
            chunks.append(chunk)

            # Create 5 similar transcriptions for each chunk
            for j in range(5):
                transcription = Transcription(
                    chunk_id=chunk.id,
                    user_id=test_user.id,
                    language_id=test_language.id,
                    text=f"This is test transcription {i}.",
                    quality=0.95,
                    confidence=0.92,
                    is_consensus=False,
                    is_validated=False,
                )
                db_session.add(transcription)

            # Update transcript_count
            chunk.transcript_count = 5
            db_session.commit()

        print(f"Created {len(chunks)} chunks with 5 transcriptions each")

        # Step 2: Trigger consensus calculation
        print("\n=== Step 2: Triggering consensus calculation ===")

        consensus_service = ConsensusService(db_session)
        chunk_ids = [chunk.id for chunk in chunks]

        results = consensus_service.calculate_consensus_for_chunks(chunk_ids)

        print(f"Consensus calculation results: {results}")

        # Step 3: Verify ready_for_export flag set
        print("\n=== Step 3: Verifying ready_for_export flags ===")

        for chunk in chunks:
            db_session.refresh(chunk)
            assert (
                chunk.ready_for_export is True
            ), f"Chunk {chunk.id} not marked ready for export"
            assert (
                chunk.consensus_quality >= 0.90
            ), f"Chunk {chunk.id} quality too low: {chunk.consensus_quality}"
            assert (
                chunk.consensus_transcript_id is not None
            ), f"Chunk {chunk.id} has no consensus transcript"
            print(
                f"Chunk {chunk.id}: ready={chunk.ready_for_export}, quality={chunk.consensus_quality:.2f}"
            )

        # Step 4: Create export batch
        print("\n=== Step 4: Creating export batch ===")

        storage_config = StorageConfig(
            storage_type="local", local_export_dir=temp_export_dir
        )
        export_service = ExportBatchService(db_session, storage_config)

        # Mock file operations for archive generation
        with patch.object(export_service, "generate_export_archive") as mock_generate:
            # Create a mock tar.zst archive for testing
            archive_path = os.path.join(temp_export_dir, "test_batch.tar.zst")

            # Create archive with chunk data using standard tarfile (no zstd compression for test)
            with tempfile.TemporaryDirectory() as temp_archive_dir:
                # Create chunks directory
                chunks_dir = os.path.join(temp_archive_dir, "chunks")
                os.makedirs(chunks_dir)

                # Add chunk files and metadata
                for chunk in chunks:
                    db_session.refresh(chunk)

                    # Copy audio file
                    chunk_audio_name = f"chunk_{chunk.id}.webm"
                    shutil.copy(
                        chunk.file_path, os.path.join(chunks_dir, chunk_audio_name)
                    )

                    # Create metadata JSON
                    consensus_transcript = (
                        db_session.query(Transcription)
                        .filter(Transcription.id == chunk.consensus_transcript_id)
                        .first()
                    )

                    metadata = {
                        "chunk_id": chunk.id,
                        "audio_file": chunk_audio_name,
                        "transcript": (
                            consensus_transcript.text if consensus_transcript else ""
                        ),
                        "metadata": {
                            "recording_id": chunk.recording_id,
                            "chunk_index": chunk.chunk_index,
                            "duration": chunk.duration,
                            "start_time": chunk.start_time,
                            "end_time": chunk.end_time,
                            "language": test_language.name,
                            "transcript_count": chunk.transcript_count,
                            "consensus_quality": chunk.consensus_quality,
                            "created_at": (
                                chunk.created_at.isoformat()
                                if chunk.created_at
                                else None
                            ),
                        },
                    }

                    metadata_file = os.path.join(chunks_dir, f"chunk_{chunk.id}.json")
                    with open(metadata_file, "w", encoding="utf-8") as f:
                        json.dump(metadata, f, indent=2, ensure_ascii=False)

                # Create manifest
                manifest = {
                    "batch_id": "test-batch",
                    "export_timestamp": datetime.now(timezone.utc).isoformat(),
                    "chunk_count": len(chunks),
                    "total_duration_seconds": sum(c.duration for c in chunks),
                    "languages": [test_language.name],
                    "format_version": "1.0",
                    "compression": "zstd",
                    "audio_format": "webm",
                }

                manifest_file = os.path.join(temp_archive_dir, "manifest.json")
                with open(manifest_file, "w", encoding="utf-8") as f:
                    json.dump(manifest, f, indent=2)

                # Create README
                readme_file = os.path.join(temp_archive_dir, "README.txt")
                with open(readme_file, "w") as f:
                    f.write("Export Batch Archive\n")
                    f.write("===================\n\n")
                    f.write(f"Chunks: {len(chunks)}\n")
                    f.write(f"Format: WebM audio + JSON metadata\n")

                # Create tar archive (uncompressed for testing)
                with tarfile.open(archive_path, "w:gz") as tar:
                    tar.add(temp_archive_dir, arcname=".")

            file_size = os.path.getsize(archive_path)
            mock_generate.return_value = (archive_path, file_size)

            # Create export batch
            batch = export_service.create_export_batch(
                max_chunks=200, force_create=True, user_id=test_user.id
            )

        assert batch is not None
        assert batch.status == ExportBatchStatus.COMPLETED
        assert batch.chunk_count == 3
        assert len(batch.chunk_ids) == 3
        print(f"Created export batch: {batch.batch_id} with {batch.chunk_count} chunks")

        # Step 5: Verify tar.zst archive contents
        print("\n=== Step 5: Verifying archive contents ===")

        assert os.path.exists(archive_path), "Archive file not created"
        assert os.path.getsize(archive_path) > 0, "Archive file is empty"

        # Step 6: Test archive extraction
        print("\n=== Step 6: Testing archive extraction ===")

        with tempfile.TemporaryDirectory() as extract_dir:
            # Extract tar.gz archive
            with tarfile.open(archive_path, "r:gz") as tar:
                tar.extractall(extract_dir)

            # Verify extracted contents
            assert os.path.exists(os.path.join(extract_dir, "manifest.json"))
            assert os.path.exists(os.path.join(extract_dir, "README.txt"))
            assert os.path.exists(os.path.join(extract_dir, "chunks"))

            # Verify manifest
            with open(os.path.join(extract_dir, "manifest.json"), "r") as f:
                manifest = json.load(f)
                assert manifest["chunk_count"] == 3
                assert manifest["compression"] == "zstd"
                assert manifest["audio_format"] == "webm"

            # Verify chunk files
            chunks_dir = os.path.join(extract_dir, "chunks")
            chunk_files = os.listdir(chunks_dir)

            # Should have 3 audio files + 3 JSON files
            webm_files = [f for f in chunk_files if f.endswith(".webm")]
            json_files = [f for f in chunk_files if f.endswith(".json")]

            assert (
                len(webm_files) == 3
            ), f"Expected 3 webm files, found {len(webm_files)}"
            assert (
                len(json_files) == 3
            ), f"Expected 3 json files, found {len(json_files)}"

            # Verify JSON metadata
            for json_file in json_files:
                with open(
                    os.path.join(chunks_dir, json_file), "r", encoding="utf-8"
                ) as f:
                    metadata = json.load(f)
                    assert "chunk_id" in metadata
                    assert "audio_file" in metadata
                    assert "transcript" in metadata
                    assert "metadata" in metadata
                    assert metadata["metadata"]["consensus_quality"] >= 0.90

            print(f"Successfully extracted and verified {len(webm_files)} chunks")

        # Step 7: Verify cleanup
        print("\n=== Step 7: Verifying cleanup ===")

        # Cleanup exported chunks
        export_service.cleanup_exported_chunks(batch.chunk_ids)

        # Verify chunks deleted
        remaining_chunks = (
            db_session.query(AudioChunk)
            .filter(AudioChunk.id.in_(batch.chunk_ids))
            .count()
        )
        assert remaining_chunks == 0, f"Expected 0 chunks, found {remaining_chunks}"

        # Verify transcriptions deleted
        remaining_transcriptions = (
            db_session.query(Transcription)
            .filter(Transcription.chunk_id.in_(batch.chunk_ids))
            .count()
        )
        assert (
            remaining_transcriptions == 0
        ), f"Expected 0 transcriptions, found {remaining_transcriptions}"

        print("Cleanup completed successfully")
        print("\n=== End-to-end export workflow test PASSED ===")


class TestStorageBackendSwitching:
    """Test storage backend switching between local and R2."""

    @pytest.mark.skip(
        reason="Requires PostgreSQL - SQLite doesn't support unnest() function"
    )
    def test_local_storage_export(
        self,
        db_session,
        test_user,
        test_language,
        test_script,
        temp_export_dir,
        temp_audio_dir,
    ):
        """Test export with local storage configuration."""
        print("\n=== Testing Local Storage Export ===")

        # Create chunks ready for export
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "recording.webm"),
            duration=10.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        chunks = []
        for i in range(2):
            audio_file_path = os.path.join(temp_audio_dir, f"chunk_{i}.webm")
            with open(audio_file_path, "wb") as f:
                f.write(b"WEBM_DATA_" + str(i).encode() * 50)

            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=audio_file_path,
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                ready_for_export=True,
                transcript_count=5,
                consensus_quality=0.95,
            )
            db_session.add(chunk)
            db_session.commit()

            # Add consensus transcription
            transcription = Transcription(
                chunk_id=chunk.id,
                user_id=test_user.id,
                language_id=test_language.id,
                text=f"Local storage test {i}",
                quality=0.95,
                confidence=0.92,
                is_consensus=True,
            )
            db_session.add(transcription)
            db_session.commit()

            chunk.consensus_transcript_id = transcription.id
            db_session.commit()
            db_session.refresh(chunk)
            chunks.append(chunk)

        # Configure local storage
        storage_config = StorageConfig(
            storage_type="local", local_export_dir=temp_export_dir
        )
        export_service = ExportBatchService(db_session, storage_config)

        # Create export batch
        with patch.object(export_service, "generate_export_archive") as mock_generate:
            archive_path = os.path.join(temp_export_dir, "local_test.tar.zst")

            # Create minimal archive
            with open(archive_path, "wb") as f:
                f.write(b"MOCK_ARCHIVE_DATA")

            mock_generate.return_value = (archive_path, os.path.getsize(archive_path))

            batch = export_service.create_export_batch(
                max_chunks=200, force_create=True, user_id=test_user.id
            )

        assert batch is not None
        assert batch.storage_type == StorageType.LOCAL
        assert os.path.exists(batch.archive_path)
        print(f"Local storage export successful: {batch.archive_path}")

        # Test download functionality
        file_path, mime_type = export_service.download_export_batch(
            batch_id=batch.batch_id,
            user_id=test_user.id,
            ip_address="127.0.0.1",
            user_agent="test-agent",
        )

        assert file_path == batch.archive_path
        assert mime_type == "application/x-tar"
        print("Local storage download successful")

    @pytest.mark.skip(
        reason="Requires PostgreSQL - SQLite doesn't support unnest() function"
    )
    def test_r2_storage_export_mocked(
        self,
        db_session,
        test_user,
        test_language,
        test_script,
        temp_export_dir,
        temp_audio_dir,
    ):
        """Test export with R2 storage configuration (mocked)."""
        print("\n=== Testing R2 Storage Export (Mocked) ===")

        # Create chunks ready for export
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "recording.webm"),
            duration=10.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        chunks = []
        for i in range(2):
            audio_file_path = os.path.join(temp_audio_dir, f"chunk_r2_{i}.webm")
            with open(audio_file_path, "wb") as f:
                f.write(b"WEBM_DATA_R2_" + str(i).encode() * 50)

            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=audio_file_path,
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                ready_for_export=True,
                transcript_count=5,
                consensus_quality=0.95,
            )
            db_session.add(chunk)
            db_session.commit()

            # Add consensus transcription
            transcription = Transcription(
                chunk_id=chunk.id,
                user_id=test_user.id,
                language_id=test_language.id,
                text=f"R2 storage test {i}",
                quality=0.95,
                confidence=0.92,
                is_consensus=True,
            )
            db_session.add(transcription)
            db_session.commit()

            chunk.consensus_transcript_id = transcription.id
            db_session.commit()
            db_session.refresh(chunk)
            chunks.append(chunk)

        # Configure R2 storage
        storage_config = StorageConfig(
            storage_type="r2",
            r2_account_id="test_account",
            r2_access_key_id="test_key",
            r2_secret_access_key="test_secret",
            r2_bucket_name="test_bucket",
            r2_endpoint_url="https://test.r2.cloudflarestorage.com",
        )
        export_service = ExportBatchService(db_session, storage_config)

        # Mock boto3 S3 client
        with patch("boto3.client") as mock_boto_client:
            mock_s3 = Mock()
            mock_boto_client.return_value = mock_s3

            # Mock upload_file
            mock_s3.upload_file.return_value = None

            # Mock generate_presigned_url
            mock_s3.generate_presigned_url.return_value = (
                "https://test.r2.cloudflarestorage.com/signed-url"
            )

            # Create export batch
            with patch.object(
                export_service, "generate_export_archive"
            ) as mock_generate:
                archive_path = os.path.join(temp_export_dir, "r2_test.tar.zst")

                # Create minimal archive
                with open(archive_path, "wb") as f:
                    f.write(b"MOCK_R2_ARCHIVE_DATA")

                mock_generate.return_value = (
                    archive_path,
                    os.path.getsize(archive_path),
                )

                batch = export_service.create_export_batch(
                    max_chunks=200, force_create=True, user_id=test_user.id
                )

            assert batch is not None
            assert batch.storage_type == StorageType.R2
            assert (
                "r2.cloudflarestorage.com" in batch.archive_path
                or batch.archive_path.startswith("exports/")
            )
            print(f"R2 storage export successful: {batch.archive_path}")

            # Verify upload was called
            assert mock_s3.upload_file.called
            print("R2 upload verified")

            # Test download functionality (should generate signed URL)
            with patch.object(export_service, "_get_r2_signed_url") as mock_signed_url:
                mock_signed_url.return_value = (
                    "https://test.r2.cloudflarestorage.com/signed-download-url"
                )

                file_path, mime_type = export_service.download_export_batch(
                    batch_id=batch.batch_id,
                    user_id=test_user.id,
                    ip_address="127.0.0.1",
                    user_agent="test-agent",
                )

                assert "signed-download-url" in file_path or file_path.startswith(
                    "https://"
                )
                assert mime_type == "application/x-tar"
                print("R2 storage download successful")

    @pytest.mark.skip(
        reason="Requires PostgreSQL - SQLite doesn't support unnest() function"
    )
    def test_storage_backend_data_format_consistency(
        self,
        db_session,
        test_user,
        test_language,
        test_script,
        temp_export_dir,
        temp_audio_dir,
    ):
        """Verify export data format is consistent across storage backends."""
        print("\n=== Testing Storage Backend Data Format Consistency ===")

        # Create test data
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "recording.webm"),
            duration=5.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        audio_file_path = os.path.join(temp_audio_dir, "chunk_consistency.webm")
        with open(audio_file_path, "wb") as f:
            f.write(b"WEBM_CONSISTENCY_TEST" * 10)

        chunk = AudioChunk(
            recording_id=recording.id,
            chunk_index=0,
            file_path=audio_file_path,
            start_time=0.0,
            end_time=5.0,
            duration=5.0,
            ready_for_export=True,
            transcript_count=5,
            consensus_quality=0.95,
        )
        db_session.add(chunk)
        db_session.commit()

        transcription = Transcription(
            chunk_id=chunk.id,
            user_id=test_user.id,
            language_id=test_language.id,
            text="Consistency test transcription",
            quality=0.95,
            confidence=0.92,
            is_consensus=True,
        )
        db_session.add(transcription)
        db_session.commit()

        chunk.consensus_transcript_id = transcription.id
        db_session.commit()

        # Test with local storage
        local_config = StorageConfig(
            storage_type="local", local_export_dir=temp_export_dir
        )
        local_service = ExportBatchService(db_session, local_config)

        with patch.object(local_service, "generate_export_archive") as mock_generate:
            archive_path = os.path.join(temp_export_dir, "consistency_local.tar.zst")
            with open(archive_path, "wb") as f:
                f.write(b"MOCK_LOCAL_DATA")
            mock_generate.return_value = (archive_path, os.path.getsize(archive_path))

            local_batch = local_service.create_export_batch(
                max_chunks=200, force_create=True, user_id=test_user.id
            )

        # Verify batch structure
        assert local_batch.chunk_count == 1
        assert len(local_batch.chunk_ids) == 1
        assert local_batch.chunk_ids[0] == chunk.id

        print("Data format consistency verified across storage backends")


class TestBulkOperationsPerformance:
    """Test bulk operations performance."""

    def test_bulk_chunk_insert(
        self, db_session, test_user, test_language, test_script, temp_audio_dir
    ):
        """Test inserting 100 chunks in single transaction."""
        print("\n=== Testing Bulk Chunk Insert (100 chunks) ===")

        # Create voice recording
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "bulk_recording.webm"),
            duration=500.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        # Bulk insert 100 chunks
        import time

        start_time = time.time()

        chunks = []
        for i in range(100):
            audio_file_path = os.path.join(temp_audio_dir, f"bulk_chunk_{i}.webm")
            # Don't create actual files for performance test

            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=audio_file_path,
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                transcript_count=0,
                ready_for_export=False,
                consensus_quality=0.0,
            )
            chunks.append(chunk)

        # Bulk insert
        db_session.bulk_save_objects(chunks)
        db_session.commit()

        elapsed_time = time.time() - start_time

        # Verify all chunks inserted
        chunk_count = (
            db_session.query(AudioChunk)
            .filter(AudioChunk.recording_id == recording.id)
            .count()
        )

        assert chunk_count == 100
        print(f"Inserted 100 chunks in {elapsed_time:.3f} seconds")
        print(f"Average: {elapsed_time/100*1000:.2f} ms per chunk")

        # Performance assertion (should be fast)
        assert elapsed_time < 5.0, f"Bulk insert too slow: {elapsed_time:.3f}s"

    def test_bulk_transcription_insert(
        self, db_session, test_user, test_language, test_script, temp_audio_dir
    ):
        """Test inserting 500 transcriptions in single transaction."""
        print("\n=== Testing Bulk Transcription Insert (500 transcriptions) ===")

        # Create voice recording and chunks
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "bulk_trans_recording.webm"),
            duration=500.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        # Create 100 chunks
        chunks = []
        for i in range(100):
            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=f"/test/bulk_trans_chunk_{i}.webm",
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                transcript_count=0,
                ready_for_export=False,
            )
            chunks.append(chunk)

        db_session.bulk_save_objects(chunks)
        db_session.commit()

        # Refresh to get IDs
        chunk_ids = [
            c.id
            for c in db_session.query(AudioChunk)
            .filter(AudioChunk.recording_id == recording.id)
            .all()
        ]

        # Bulk insert 500 transcriptions (5 per chunk)
        import time

        start_time = time.time()

        transcriptions = []
        for chunk_id in chunk_ids:
            for j in range(5):
                transcription = Transcription(
                    chunk_id=chunk_id,
                    user_id=test_user.id,
                    language_id=test_language.id,
                    text=f"Bulk transcription for chunk {chunk_id}, version {j}",
                    quality=0.90 + (j * 0.01),
                    confidence=0.85 + (j * 0.02),
                )
                transcriptions.append(transcription)

        # Bulk insert
        db_session.bulk_save_objects(transcriptions)
        db_session.commit()

        elapsed_time = time.time() - start_time

        # Verify all transcriptions inserted
        trans_count = (
            db_session.query(Transcription)
            .filter(Transcription.chunk_id.in_(chunk_ids))
            .count()
        )

        assert trans_count == 500
        print(f"Inserted 500 transcriptions in {elapsed_time:.3f} seconds")
        print(f"Average: {elapsed_time/500*1000:.2f} ms per transcription")

        # Performance assertion
        assert elapsed_time < 10.0, f"Bulk insert too slow: {elapsed_time:.3f}s"

    def test_prioritized_chunk_query_performance(
        self, db_session, test_user, test_language, test_script, temp_audio_dir
    ):
        """Test query performance for prioritized chunks with 10K+ chunks."""
        print("\n=== Testing Prioritized Chunk Query (10K chunks) ===")

        # Create voice recording
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path=os.path.join(temp_audio_dir, "perf_recording.webm"),
            duration=50000.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        # Create 10,000 chunks with varying transcript_counts
        print("Creating 10,000 chunks...")
        chunks = []
        for i in range(10000):
            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=f"/test/perf_chunk_{i}.webm",
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                transcript_count=i % 10,  # Vary from 0-9
                ready_for_export=False,
                consensus_quality=0.0,
            )
            chunks.append(chunk)

            # Commit in batches to avoid memory issues
            if len(chunks) >= 1000:
                db_session.bulk_save_objects(chunks)
                db_session.commit()
                chunks = []

        if chunks:
            db_session.bulk_save_objects(chunks)
            db_session.commit()

        print("Chunks created, testing query performance...")

        # Test prioritized query (ORDER BY transcript_count ASC)
        import time

        start_time = time.time()

        results = (
            db_session.query(AudioChunk)
            .filter(AudioChunk.ready_for_export == False)
            .order_by(AudioChunk.transcript_count.asc())
            .limit(50)
            .all()
        )

        elapsed_time = time.time() - start_time

        assert len(results) == 50
        assert results[0].transcript_count <= results[-1].transcript_count

        print(f"Query completed in {elapsed_time:.3f} seconds")
        print(f"First chunk transcript_count: {results[0].transcript_count}")
        print(f"Last chunk transcript_count: {results[-1].transcript_count}")

        # Performance assertion (should be < 1 second with proper indexing)
        assert (
            elapsed_time < 1.0
        ), f"Query too slow: {elapsed_time:.3f}s (needs indexing)"

    def test_index_usage_verification(
        self, db_session, test_user, test_language, test_script
    ):
        """Verify index usage with EXPLAIN ANALYZE."""
        print("\n=== Testing Index Usage ===")

        # Create some test data
        recording = VoiceRecording(
            user_id=test_user.id,
            script_id=test_script.id,
            language_id=test_language.id,
            file_path="/test/index_test.webm",
            duration=50.0,
            status=RecordingStatus.CHUNKED,
        )
        db_session.add(recording)
        db_session.commit()

        # Create chunks
        chunks = []
        for i in range(100):
            chunk = AudioChunk(
                recording_id=recording.id,
                chunk_index=i,
                file_path=f"/test/index_chunk_{i}.webm",
                start_time=i * 5.0,
                end_time=(i + 1) * 5.0,
                duration=5.0,
                transcript_count=i % 10,
                ready_for_export=(i % 5 == 0),
                consensus_quality=0.8 + (i % 20) * 0.01,
            )
            chunks.append(chunk)

        db_session.bulk_save_objects(chunks)
        db_session.commit()

        # Test EXPLAIN for prioritized query
        explain_query = text(
            """
            EXPLAIN QUERY PLAN
            SELECT * FROM audio_chunks
            WHERE ready_for_export = 0
            ORDER BY transcript_count ASC
            LIMIT 50
        """
        )

        result = db_session.execute(explain_query)
        explain_output = result.fetchall()

        print("Query plan:")
        for row in explain_output:
            print(f"  {row}")

        # For SQLite, check if index is mentioned in query plan
        str(explain_output).lower()

        # Note: SQLite may use different index strategies
        # We're mainly checking that the query executes and has a plan
        assert len(explain_output) > 0, "No query plan generated"
        print("Index usage verified (query plan generated)")

        # Test EXPLAIN for export batch query
        explain_query2 = text(
            """
            EXPLAIN QUERY PLAN
            SELECT * FROM audio_chunks
            WHERE ready_for_export = 1
            LIMIT 200
        """
        )

        result2 = db_session.execute(explain_query2)
        explain_output2 = result2.fetchall()

        print("\nExport batch query plan:")
        for row in explain_output2:
            print(f"  {row}")

        assert (
            len(explain_output2) > 0
        ), "No query plan generated for export batch query"
        print("Export batch query index usage verified")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
